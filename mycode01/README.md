## 4. 쿠버네티스 아키텍쳐 
![](https://d33wubrfki0l68.cloudfront.net/2475489eaf20163ec0f54ddc1d92aa8d4c87c96b/e7c81/images/docs/components-of-kubernetes.svg)
쿠버네티스는 홀수댓수의 마스터노드와 여러대의 워커노드로 구성된다.  
마스터는 컨트롤 플레인이라고도 부름  
책에서 보여주는 컴포넌트 apiserver,scheduler,controller-manager. kubelet, proxy 등은 마스터 노드에서 각각이 해당 컴포넌트를 실행하고, 컴포넌트들끼리 고가용성을 유지하기 위해 통신하고 있는 형태로 구현된다. 즉 마스터노드가 늘어나면 새로 붙은 마스터노드도 위의 컴포넌트를 각각 실행하고 다른 마스터노드와 통신한다.  
마스터노드가 홀수인 이유는 raft 알고리즘을 통해 리더 마스터노드가 장애로 죽었을때 새 리더를 선출하기 위해 투표를 참여하는데, 짝수일때는 과반을 만들기 어렵기 때문이라 한다. 단순히 생각해보자면 차이가 없을거같은데, 알고리즘의 간결함을 위해 그렇다 함.  

##### apiserver
쿠버네티스의 API를 제공하는 컴포넌트. 각 컴포넌트는 대부분 컨테이너로 동작하고 있다.  
kubectl을 통해 사용자가 명령을 쏘면 처리를 해주는 역할을 한다.(web ui로도 가능)  

##### etcd
쿠버네티스가 구동되며 필요한 데이터를 저장하는 kv 저장소.  

##### scheduler
팟을 생성하기 위해 워커노드를 감지하는 스케줄러.   
스케쥴링을 한다고 하면 약간 이상하게 들릴수 있지만 그러려니 하자.  
느낌이 잘 안온다면 자바에서 스레드를 이용하기 위해 스케쥴러를 부르는것을 생각하면 될듯.  
암튼 총합적으로 팟이 필요한 시스템 리소스나 데이터 지역성, 워크로드 간섭등을 판단해서 적절한 노드에 팟을 올리기 위한 컴포넌트이다.  

##### controller-manager 
파드를 실행시키기 위한 컨트롤러...를 컨트롤하는 매니저  
쿠버네티스는 ReplicaSet, Deployment, StatefulSet, DaemonSet 등의 컨트롤러를 지원하는데, 이런 컨트롤러의 상태를 모니터링하고 매니징하는 컴포넌트. 뭔말인지 모르겠지만 6장에 나옴   

##### kubelet 
마스터노드로부터 할당된 팟과 컨테이너를 매니징하기 위한 컴포넌트.  

##### proxy
네트워크 프록시,   
proxy 는 각 파드에게 외부트래픽을 전달하는 역할을 함.  
 
##### container runtime
컨테이너 런타임.   
실제로 컨테이너가 실행되기 위한 런타임이며, Docker도 그 런타임중 하나임.  
Docker, cri-o, containerd 등이 있음. 몇년전 Docker가 OCI라는 런타임 규격을 지키지 않아 문제가 있었음. shim 이라는 중간계층을 두어 docker를 지원하였으나, 치워버림. 현재는 Docker-engine을 통해 docker를 지원하는듯?  
디폴트는 containerd   

##### DNS
기본 dns가 있긴 하지만 확장성, 호환성등을 이유로 DNS애드온을 사용한다고 함.  
일반적인 DNS 기능을 쿠버네티스 환경에서 사용하기 위해 제공되는 듯  

##### ingress controller
ingress 를 지원하기 위한 컨트롤러.  
ingress 는 외부 트래픽을 내부 네트워크와 연결해주기 위해 사용되는 기능.  
라우팅 룰에 따라 서비스(내부 라우터)에게 전달해주는 역할을 함.  
![](https://d33wubrfki0l68.cloudfront.net/0e185f84ae43dcdc8952eb7d8f98c3eba87d2e3a/05337/ko/docs/images/ingress.svg)

---

### 오브젝트와 컨트롤러
쿠버네티스는 오브젝트와 컨트롤러가 있음  

아래는 오브젝트  
##### pod
쿠버네티스에서 실행되는 최소 단위. 격리되어있음.  
서비스를 구동하는데 사용되는 최소한의 단위. 독립적인 공간과 ip를 가지지만, 여러 이유에 의해 팟은 계속 사라졌다가 다시 뜨게 되므로 ip라는게 별 의미가 없음.  
한개의 팟에는 여러 컨테이너가 들어갈수 있지만 보통 하나의 팟에 하나의 컨테이너로 사용한다고 함.  

##### 네임스페이스
클러스터에서 사용되는 리소스들을 구분짓기 위한 관리 단위 그룹.  
디플로이먼트, 서비스, 파드 등에 적용되며, 노드같은 클러스터 범위의 오브젝트에는 적용되지 않음.  
디폴트가 있긴 하지만 관리측면에서는 네임스페이스를 사용하는것이 좋음.  

##### 볼륨 
다들 아시는 그냥 볼륨.  
팟은 계속 생성되고 사라지고가 컨트롤러에 의해 관리되므로 내부적으로 사용하는 디렉터리도 마찬가지로 임시적이므로 파드가 사라지더라도 사용할수 있어야함.  
그러기 위한 볼륨 오브젝트.  

##### 서비스
팟은 계속 생성되고 사라지고가 컨트롤러에 의해 관리되므로 접속 정보등이 고정되지 않음.   
ip를 알아야 할 필요가 없지만 반대로 그 정보가 고정되어 알수가 없다는 뜻.  
이 접속을 유지하기 위한 디스커버리 혹은 로드밸런서, 혹은 그 팟의 집합.  

##### 컨트롤러
클러스터 내 팟을 관리하고 사용자가 지정한 상태를 유지하기 위한 컴포넌트.  
지속적으로 팟을 모니터링하고, 팟에 관해 사용자의 명령을 수행함.  
몇가지 유형이 있음.  

---

### pod
앞에서 설명한대로 팟은 작업의 최소 단위.  
사실 더 작은 단위인 컨테이너가 존재하긴 하지만 쿠버네티스가 관리하는 최소 단위가 팟이고, 관리 측면에서 1개의 컨테이너 = 1개의 팟으로 이용한다고 함.(p126)  
해당 페이지 설명을 보면 하나의 파드안에 컨테이너를 여러개 넣은 상황에 대해 설명을 하는데,  
격리된 하나의 vm 이라고 생각하면 쉽게 이해할수 있을거같다.  
사실 우리가 aws 나 gcp에서 사용하는 컴퓨팅자원은 실제 머신이 아니라 vm이고, 아마 내부적으로는 쿠버네티스처럼 관리되고 있지 않을까 함.(그래서 잘 보면 공유 컴퓨팅 자원 이라고 명시함)  

q. 그렇다면 우리가 굳이 쿠버네티스를 통해 서비스를 제공할 필요는 없지 않느냐? 오히려 컴퓨팅 자원은 한정적인데 쿠버네티스 환경에 의해 손해를 보는게 아닌가?  
a. 맞음. 아이피에 의해 수동으로 디스커버리되는, 잦은 수평적 확장이 필요없는 환경에서는 굳이 쿠버네티스를 사용함으로 손해를 볼필요는 없음.   
하지만 나의 어플리케이션이 수평적으로 확장될 준비가 되어있고, 그런 요구(demand)가 있고, 마케팅 워드가 필요하다면 사용하면 됨.  

##### pod의 생명주기(giving life아님ㅎ)
팟은 생명 주기를 가지고 있고, `kubectl describe pods POD_NAME` 명령으로 확인 가능함.
상태는 다음과 같다.  
- pending
노드에 파드를 생성하는 도중. 아직 일을 하고있지는 않음. 컨테이너 이미지를 다운받는 과정을 포함함.  
- running
팟의 내부 컨테이너가 정상 동작중.   
- succeeded
정상 실행 종료.  
- failed
어떤 이유로 정상종료되지 않고 실패코드로 종료된 경우.  
- unknown
몰?루  
시스템이 파드의 상태를 확인할수없는 경우. 보통은 통신불량


##### probe
주기적으로 kubelet이 파드 안의 컨테이너 상태를 진단하여 컨테이너 재시작 혹은 종료 시킨다.  
프로브는 세가지를 소개하고 있는데, 이것도 자동은 아니다. yml에 정의 해야함.  
- liveness
컨테이너가 살아있는지 감지하는 프로브
- readness
컨테이너가 요청을 처리할수 있는 상태인지 감지하는 프로브
- startup
컨테이너가 시작되었는지 확인하는 프로브  
  
위 프로브들이 컨테이너를 확인하는 방법은 몇가지 있는데,
- Exec
컨테이너에 특정 명령을 실행시켜 확인
- TCP socket
특정 tcp 포트를 찔러보고 확인
- HTTP GET 
http 요청을 보내 200~400 이 나오는지 확인
- gRPG
gRPC를 사용하여 프로시저 호출, 응답이 SERVING인지 확인


##### 초기화 컨테이너
팟 내 나의 컨테이너가 실행되기 전에 실행되는 컨테이너.  
자바에서 객체 만들때 constructor 실행되는거라고 생각하면 될거같음.  
이걸 어따써? 라고 생각할수 있지만, 내가 구동할 어플리케이션이 먼저 필요한 파일이 용량이 큰게 있다면?  
컨테이너 이미지에 포함시키는 방법도 있으나, 파일이 바뀔때마다 이미지도 업데이트를 해야하고, 이를 보관하는것도 부담스럽다.  
이때 초기화 컨테이너를 이용하여 원하는 곳에서 파일을 받아오고 나의 어플리케이션을 수행시키는 방법등..  
만약, 초기화 컨테이너가 작업에 실패한다면 yml 설정에 따라 다르긴하지만 파드 작업 전체가 중지된다.


##### static pod
kubelet을 통해 직접 실행되는 파드가 있는데, 이를 스태틱 팟이라 부름. 주로 시스템쪽에 이용되는 파드다.  
kubelet에 의해 관리됨. apiserver를 거쳐 조종할수 없음.  


##### 자원 할당
pod도 결국은 노드위에서 동작하는거라, 고사양이 필요한 팟이 한 노드에 집중되면 문제가 발생함.  
스케줄러가 알아서 할당해주긴 하지만 기계적으로 처리되는거라 완벽하지 않음.   
yml 정의시 컨테이너가 어느정도의 자원을 사용할수 있는지 지정할수 있음.  
- .spec.containers[].resources.limits.cpu
시피유 한계량
- .spec.containers[].resources.limits.memory
메모리 한계량
- .spec.containers[].resources.requests.cpu
최소 cpu 자원 요구량
- .spec.containers[].resources.request.memory
최소 메모리 자원 요구량
만약, request 만큼의 여유자원이 있는 노드가 없다면 팟은 실행되지 못하고 pending상태로 쭉 대기하게 됨.  
limit 을 설정했기 때문에 오버커밋 상태가 발생할수있는데, 이 경우 우선순위가 낮은 컨테이너를 죽이고 필요하다면 팟을 다른 노드에 옮겨 실행할수도 있다고 한다.  
사용자 입장에선 아무 이유없이 파드가 죽었다고 생각할수도 있겠다.  
잘 모르겠다면 그냥 request와 limit을 동일하게 설정하라고 함.  


패턴은 뭔소린지도 모르겠으니 넘어갑니다.

---

### 컨트롤러 
팟를 직접 관리하는 것은 귀찮다. 노드에 장애가 생겼을때 내가 원하는 상태를 유지시키는것 또한 수동으로 해야하기 때문. 이럴거면 쿠버네티스를 쓸 이유가 없다.  
그렇기 때문에 관리를 해주는 컴포넌트가 필요한데, 이것이 컨트롤러.  
쿠버네티스에서는 팟의 관리목적으로 여러 컨트롤러를 지원함.  

대충 책을 읽었다는걸 가정하고 설명하자면
##### 상태 관리가 필요한 파드
스테이트풀 세트  
이름에서부터 상태에 관해 이야기함. 상태를 가지는 어플리케이션을 관리하기 위한 컨트롤러인데,  
영속성을 가지는 디비라던지 메시지 큐(볼륨사용), 혹은 파드의 고유성이 필요할때 사용한다.  
예를들면 IP로 직접 통신이 필요한경우(실제로 그렇다는 이야기가 아니라, 이름을 지정해야할때)  
같은일을 하는 어플리케이션이지만 각 어플리케이션마다 통신되는 목적지가 다른경우라거나.   
스테이트리스와 비교하면 이해가 쉬울듯   

##### 상태 유지를 하지 않아도 되는 파드
레플리케이션 컨트롤러(RC)  
레플리카 세트(RS)  
디플로이먼트  
rc는 쿠버네티스 초기부터 있던 기능이라 rs와 비슷하지만 이전버전에 가깝다.  
지정된 수의 레플리카를 생성하고 상태를 관리한다는 면에서는 비슷하지만 레플리카셋은 레이블 셀렉터를 이용할수 있다는 점에서 좀 더 유연하다. 특정레이블을 가진 파드만 확대/축소가 가능함.  
하지만 쿠버네티스 초기부터 있던 rc는 롤링업데이트 기능이 있지만, rs는 없음.   
아마 디플로이와 역할을 나누어 좀더 유연한 기능을 제공하려고 했던게 아닐지?  
디플로이먼트는 rs보다 좀 더 추상화된? 좀더 상위 개념이라고 볼수있을거같음.  
그냥 단순하게 rs에 배포와 롤아웃을 추가한 컨트롤러임.  
rs의 기능을 포함하고 있고, 스케일링과 배포에 관련된 롤아웃, 롤백 기능을 제공함.  

##### 클러스터 내부 공통처리를 위한 파드
데몬셋   
클러스터 내 모든 노드에 공통적으로 실행되어야하는 파드를 관리하는 컨트롤러  
노드마다 로그수집기, 시스템 리소스 모니터등이 여기에 포함된다.  

##### 일회성 작업을 위한 파드
잡  
크론잡  
잡은 aws ecs의 task라고 볼수 있을거같고, 크론잡은 잡에 cron을 이용하여 수행시키는 작업이다.  
잡의 종류는 단일잡, 병렬잡이 있다.  
단일잡은 말 그대로 단일작업을 수행하고 완료시키는 잡,  
잡 yml 정의시 completion에 1이상 넣으면 단일 잡을 차례로 수행하며, 정의한 수만큼 수행하고 끝남.  
병렬잡은 여러개의 파드로 이루어진 잡이다. yml 정의시 parallelism 값으로 동시성 정의가 가능함.  
말그대로 여러개 작업이 같이 수행됨.   
크론잡은 그냥 잡을 cron expression에 맞게 정의하면 컨트롤러가 해당 시간에 맞게 잡을 수행시켜줌.  
일반적으로 배치가 이런식으로 수행됨.  
deadlineSeconds 설정으로 수행제한시간을 설정할수 있음.  

---

### 서비스
서비스는 팟에 네트워크를 통해 접근해야할 방법을 제공함  
팟은 여러 워커 노드에 무작위적으로 생성될수 있고, 고정된 ip를 제공할수 없기 때문에 연결할 방법을 제공해야함  
또한, 팟은 같은 종류가 여러개 떠있을수 있으므로 고정되고 단일화된 네트워크 진입점을 제공해야함 

- ClusterIp
  - 기본 서비스 타입  
  - 내부 네트워크에서는 이 ClusterIp 를 통해 파드끼리의 통신이 이루어짐  
  - 외부 네트워크 기능이 없음. 오직 내부 통신용  
- NodePort
  - 노드에 들어오는 요청을 포트와 연결된 파드로 전달
  - 해당 팟이 떠있지 않은 노드를 때려도 팟이 떠있는 노드로 요청이 전달됨
  - 노드를 통해 외부 네트워크 기능 제공 
- LoadBalancer
  - 클라우드 서비스의 로드밸런서와 연결됨 
  - 클러스터 외부의 장비에 연결되기 때문에 NodePort, ClusterIp, 로드밸런싱 알고리즘은 클라우드 제공자의 정책에 따름
- ExternalName
  - Ip대신 DNS name을 직접 사용할때 사용함 
  - CNAME과 같은 역할

yml 작성시 .spec.clusterIp 항목에 직접 원하는 IP를 지정할수 있음. 해당 항목이 없다면 자동생성
.spec.selector 에서 팟에 레이블로 지정한 이름으로 팟을 선택해 서비스와 연결시킬수 있음

##### 헤드리스
단순히 ClusterIp를 NONE으로 설정해 ClusterIp 생성을 막고, IP를 통한 접근을 막을수 있음.
파드가 여러개 있어도 A record를 통해 접근해야함.
왜 필요한지? 스테이트풀셋 설명때,  
팟 마다 연결되는 목적지가 다른경우(같은 팟이래도 행동이 다를수 있을때), 단일한 진입점이 필요 없을때(단일 진입점을 노출하고싶지 않을때)

##### kube-proxy
클러스터 내에서 실제 디스커버리와 로드밸런싱을 담당하는 컴포넌트  
워커마다 내부에 팟으로 동작함(daemon set)  
팟은 무작위적으로 여러 노드에서 실행되고 사라지며 유동적이기 때문에 ip를 물고 통신하는것은 의미가 없음  
이것이 실시간으로 관리되기 위해서 누군가, 혹은 무엇인가가 통신을 위해 라우팅 처리를 해줄 필요가 있음  
주로 팟에 대한 디스커버리, 로드밸런싱, 포트포워딩등의 작업을 함  

로드밸런싱 타입은 다음과 같다  
- userspace
  - 팟으로의 요청이 iptable을 거쳐서 옴
  - 라운드로빈
- iptables
  - 위와 비슷하지만 kube-proxy가 요청을 직접 처리하지 않고 내부 iptable만 관리함
  - iptable을 거쳐 바로 팟으로 전달됨
- IPVS
  - L7
  - 커널레벨에서 동작하여 성능이 좋음
  - 여러 로드밸런싱 알고리즘을 지원 

---

### 인그레스
서비스와 유사하지만 좀 다름  
결정적인 차이는 L7이기 때문에 http/https를 기본제공하고, 그 외에는 커스텀 인그레스 컨트롤러를 사용하고 설정해야함  
서비스는 L4이기 때문에 커스텀 포트도 사용 가능함  
서비스가 단일 진입점(사용자 입장에서의)을 제공한다면   
인그레스는 클러스터 외부에서 내부로 들어오는 트래픽을 처리하는 기능에 특화되어 있음  

서비스는 팟 간의 통신을 위한 네트워크 추상화를 제공  
인그레스는 주로 외부 내부 트래픽 관리, 라우팅(주소에 따른 라우팅기능)  

인그레스는 외부에 노출되는 IP를 가짐  
또한 인그레스는 컨트롤러에 따라 다르지만 vhost 설정도 가능  
SSL 인증서 설정 기능도 가짐(마찬가지로 컨트롤러에 따라 다를듯)  
표준화 되어있는건지는 모르겠습니다  

---

### 레이블
레이블은 클러스터내에서 오브젝트를 구분하는 용도로 사용함(오브젝트와 컨트롤러 참고)  
yml에서 셀렉터를 이용해 타겟을 선택할때 사용됨  
키=값으로 구성  
레이블 셀렉터로 in notin 등으로 그루핑할수 있고, 이를 통해 여러 팟을 한꺼번에 선택, 제외가능
또한 레이블은 여러개를 가질수 있으므로 일부만 선택하거나 제외를 할수도 있음.

### 어노테이션
레이블과 유사함  
하지만 셀렉터를 이용한 방법으로 이용되지는 못함  
주로 정보전달을 위한 메타데이터로서 존재함  
일부 컨테이너는 이 애너테이션값을 가지고 설정값처럼 사용할수는 있지만 이후에 나올 컨피그맵이 있기 때문에..  

---

### 컨피그맵
테스트환경과 운영환경을 별도로 가진 서비스에서는 관리상 편의성을 이유로 컨테이너(프로그램)과 환경설정을 분리해서 관리한다  
환경설정이 변경된다 해서 컨테이너 이미지를 새로만드는것은 번거롭고 관리의 귀찮음도 있기 때문  
예를들어 테스트환경용 컨테이너 이미지, 운영환경용 컨테이너 이미지를 별도로 관리하는것은 낭비이며,  
문제가 생겼을때도 사소한 버전문제가 발생할수 있기 때문에 둘의 이미지는 동일하게 가져가고 환경설정만 바꿔 끼우는 경우가 많다  
쿠버네티스는 이를 좀 더 쉽게 관리할수 있는 방법인 컨피그맵을 제공한다  
yml의 .data 하위 필드로 관리되며, 키:값 으로 이루어져있다  
컨피그맵을 배포 후 파드에 적용할때에는
```
.spec.containers[].env[].valueFrom:
	configMapKeyRef:
		name: config-dev -- 컨피그맵 메타데이터 name
		key: DEBUG_INFO -- 컨피그 맵에 지정한 key
혹은
.spec.envFrom:
- configMapRef:
	name: config-dev 
```
형태로 적용하며, 배포된 팟을 desc 해보면 적용되어있음을 확인가능함  

### 시크릿
시크릿은 컨피그맵과 유사하지만 민감한 정보를 저장하는 별도의 용도이다  
용량 제한이 있으며 총 데이터가 아닌 개별데이터마다 1메가의 제한을 가짐  
기본적으로 yml 작성시 base64로 인코딩해서 넣어야하지만, stringData로 인코딩없이 넣을수도 있다  
기본 타입은 Opaque 이며, 키=값 형식  
그 외 타입으로 몇가지 있는데, dockercfg, ssh-auth, tls 정도가 눈여겨볼만하다.  
kubernetes.io/ 접두어는 쿠버네티스에서 사용하는 값임을 알리는 문자열이며, 관례에 가깝다.  

사용법 자체는 컨피그맵과 동일하지만 configMapKeyRef, 대신 secretKeyRef 를 사용한다는 점이 다르다.  

--- 


### pod scheduling
##### 노드 셀렉터와 어피니티
원하는 노드에 팟을 실행시킬수 있음. 노드에도 라벨을 설정할수 있는데, 특정 노드에 라벨을 설정하고, 팟 yml 정의시에 spec.nodeSelector에 원하는 노드의 라벨을 설정하면 됨.  
노드 셀렉터 말고도 노드 어피니티라는 개념이 있는데, 일종의 스케쥴링 힌트라고 생각됨.  
어떤 레이블 조건을 만족해야하는지, 아니면 되도록이면 만족하는게 좋다는 힌트를 팟 yml에 정의하고, 일치하는 노드가 있다면 해당 노드에서 실행할수 있게하는 힌트.  
반대로 안티 어피니티 라는 개념도 있는데, 노드 하나에 서로 피해야하는 팟들이 몰리지 않도록 관계성을 정의해서 다양한 노드에 배포할수 있게 도움을 줄수 있다.  
같은 팟끼리는 한 노드에 배포된다거나 특정한 팟과 같은 노드를 사용하지 못하게 설정할수 있음.  
spec.affinity.nodeAffinity  
spec.affinity.podAntiAffinity 로 정의함.

##### 테인트 
특정 노드에 팟을 더이상 배포하고 싶지 않다면 taint 설정을 하면된다.    
여기에 예외로 toleration 설정으로 특정 팟만 배포하게 할수도 있다.  
어떤 노드가 특별한 속성을 가진 장비일때, 어떤 파드가 독점적으로 노드를 사용해야할때 사용함.  
테인트는 kubectl taint nodes 노드명 키=값:효과 로 정의하고,  
톨러레이션은 spec.tolerations 로 정의함. 원하는 노드의 키와 밸류를 맞춰주면된다.  
테인트의 효과는 NoSchedule, PreferNoschedule, NoExecute 가 있다.
- NoSchedule
    톨러레이션 설정없으면 파드 스케쥴링 거부
- PreferNoSchedule
    위와 유사하지만 클러스터 내 다른 노드의 사정이 안된다면 파드 스케쥴링을 해줌
- NoExecute
    NoSchedule과 유사하지만 살짝 다르게 해당 설정을 하는 순간 기존파드라도 톨러레이션 설정이 없다면 종료시켜버림

##### 커든과 드레인
둘다 노드를 관리하기 위해서 사용되는 명령  
커든은 테인트와 비슷하지만 다르게 더이상 스케쥴링을 허용하지 않는 명령임.  
테인트가 톨러레이션과 짝이되어 특정 팟만 배포하려고 사용하지만 커든은 관리 측면에서 사용하지 못하게 하는 방식  
> kubectl cordon(uncordon) 노드명  

드레인은 특정 노드에서 실행되는 팟을 쫒아내는 명령임  
해당 노드에서 실행되는 팟은 다른 노드로 옮기게 하지만 데몬셋에 의해서 실행된 팟은 별도이므로 --ignore-daemonsets=true 플래그를 주던지 데몬셋으로 관리해야함 스태틱 파드도 마찬가지  
> kubectl drain(uncordon) 노드명 

해당 명령을 받은 노드는 스케쥴링 대상이 되지도 않고, 이전이 완료된 팟은 삭제함


--- 


### 인증과 권한
##### 인증 
apiserver는 테스트 목적으로 localhost:8080을 열어둠  
이를 사용하지 않는 정석적인 방법은 6443을 이용한 tls 인증을 통한 방법  
계정은 사용자 계정, 서비스 계정으로 나눠져있고   
서비스 계정은 쿠버네티스가 관리하는 사용자 계정 비번이 있음   
사용자 계정은 사용자 정보를 저장하지 않기 때문에 외부인증 시스템을 이용한다고 함. 무슨말인지?  

##### 권한 
인증과 별도로 권한이 따로 있음(kind:Role)  
role base access control(rbac)을 사용하며 흔하게 사용하는 사용자와 롤을 분리한 형태의 권한 부여이다.  
롤을 생성하고, 해당 롤이 무슨 리소스와 verb를 사용할수있는지 정의할수 있음.  
또한, resourceNames를 지정하여 지정된 리소스에만 접근하도록 할수있다.  
verb는 create, get, list, update, patch, delete, deletecollection, watch가 있음  
이를 조합하여 어떤 리소스에 어떤 행위를 할수 있는지를 정의한다.
이는 특정 네임스페이스에 지정하는것이고,  
kind:ClusterRole도 있는데 이것은 클러스터 전체에 대한 사용권한이다.  

위에서 말한것처럼 인증과 권한이 별도이므로, 사용자와 롤을 묶어주는 과정이 필요한데 이것이 롤 바인딩이라고 함  
그냥 일반적으로 사용하는 사용자별 롤 지정이다. 클러스터 롤 바인딩, 그냥 롤 바인딩 두개 있음  

--- 

### 데이터 저장
##### 볼륨
컨테이너가 종료되면 해당 컨테이너에 보관중이던 데이터도 사라지는데, 이를 막기위해 볼륨을 사용함.  
플러그인마다 다르지만 볼륨은 여러 파드에서 공유될수 있음.  
볼륨 마운트 옵션중 spec.container.volumeMounts.mountPropagation이 있는데,
- None
    호스트에서 볼륨으로 지정한 디렉토리만 컨테이너에서 확인 가능함  
    호스트에서도 컨테이너가 그 안에서 설정한 마운트를 볼수 없다는데.. 왜 쓰는건지?
- HostToContainer
    호스트에서 마운트 시켜준 다른 디렉토리도 컨테이너에서 확인 가능함  
- Bidirectional 
    위와 유사하지만 같은 노드의 다른 컨테이너에서도 같은 볼륨을 사용가능함 

##### 볼륨 플러그인
책에서 몇가지 플러그인을 소개함
- emptyDir 
    호스트의 디스크를 컨테이너에서 간단하게 볼륨으로 사용하는 용도.  
    팟이 사라지면 같이 사라짐 
- hostPath
    emptyDir과 유사하지만 호스트의 파일도 공유할수 있는 것같음.  
    파일이 호스트와 공유되기 때문에 활용도가 좀 더 높을것. 
- nfs(network file system)
    그냥 흔하게 사용하는 nfs 클라이언트  
    여러 파드에서 볼륨 하나를 공유해야할때 nas를 많이 쓰는데, 이것을 마운트 할수있음.  
    nfs 클라이언트인만큼 실제로 사용하기 위해서는 nfs 서버를 띄우고, nfs 서버는 어딘가의 볼륨을 마운트하고(저장소로 쓸 공간), 필요한 파드에서 nfs 서버를 마운트해 사용해야함.

##### 퍼시스턴트 볼륨
퍼시스턴트 볼륨은 볼륨 그 자체이며 퍼시스턴트 볼륨 클레임은 사용자가 요청을 보내는 대상이다  
일종의 컨트롤러 관계라고 생각하면 될거같음  
클러스터 내 리소스로 취급된다. 리소스이기 때문에 생명주기를 가짐.  
pv는 생명주기를 가지는데, 프로비저닝, 바인딩, 사용, 반환 의 단계를 순환한다.  
- 프로비저닝
    pv를 생성하는 단계. 미리 만들어두고 사용하는 방법, 요청때마다 만드는 방법이 존재함.  
    정적 사용은 미리 '용량제한'이 걸린 pv를 만들고 제공하는 방법이며  
    동적 사용은 최대 용량제한이 있는게 아니라면 그냥 만들어주는 방식인거같다.
- 바인딩 
    요청된 pv를 pvc가 가져오는 단계  
    만들었거나 가져왔다면 pvc와 연결되고 사용자에게 전달됨  
- 사용
    pvc가 팟에 연결되고 볼륨으로 인식되어 사용될수 있는 단계  
    사용중인 스토리지를 삭제할 방법은 없음 이래서 퍼시스턴트 라고 부르는듯?
- 반환
    사용이 끝난 pvc는 삭제되고 pv를 초기화하는 과정  
    여기서도 3가지 정책이 있는데,
    - retain
        pv를 보존함, 데이터도 남아있기 때문에 남은 데이터 처리의 책임은 사용자에게
    - delete
        pv를 삭제하고 실제 스토리지의 볼륨도 삭제함. default 정책
    - recycle 
        pv에서 데이터를 삭제하고 새로운 pvc와 바인딩될수 있도록함.


---

### 클러스터 네트워킹
팟은 ip로 통신하고 관리됨. 그래서 각 팟마다 ip를 할당해야함. 노드가 여러개일수 있기 때문에 호스트의 네트워크 구성을 그대로 사용하면 안됨.  
쿠버네티스의 네트워크는 팟 단위로 관리됨. 팟마다 ip를 할당하고 nat를 통해 각 팟에 접근이 가능함.  
같은 팟에 할당된 컨테이너들은 같은 ip로 묶임  
노드마다 여러대의 팟이 뜰수있는데, 노드마다 kubeproxy를 통해 ip를 할당할수는 없고(물리적으로 노드가 다르므로 어떤 아이피를 사용했는지 관리하기가 힘듬) 노드마다 대역대를 분리해서 10.10.1.0/24, 10.10.2.0/24 이런식으로 대역대를 가지고 라우팅을 함  
그리고 서비스와 팟은 별도의 네트워크 구성을 가짐... 복잡  
네트워크 플러그인은 여러가지가 있는데, 책에서는 세가지를 이야기함. 아마 인지도를 가지고 나열한거같음.  
칼리코, 플라넬, 실리엄이 있고 디폴트는 칼리코 

### dns
클러스터 내부에서 dns 설정이 가능함.  
ip가 아니라 도메인 이름으로 통신할수 있고, 내부 도메인을 이용하면 ip의존적이지 않은 앱을 구성할수 있게 됨.  
coreDNS를 사용하고, 클러스터 내부에서 팟으로 동작함.  
coreDNS는 컨피그맵에 있는 corefile을 보고 관리함. 해당 데이터에 명시된 플러그인의 순서를 따라 명령을 처리한다는데..   
암튼 쉽게 상상할수 있듯 dns 질의는 다음 순서로 이루어짐  
파드 > 클러스터 내부 dns캐시 > 클러스터 외부 호스트 dns > 호스트가 바라보는 외부 dns

dns를 사용자가 직접 설정할수 있는데, pod yml 정의할때 같이 정의한다. 그래야 팟이 통신할 대상을 정할수 있어서 그런거같음.  
spec.dnspolicy  
spec.dnsConfig 

--- 


### 로깅
클러스터 환경에서는 로그 확인이 어렵다. 팟이 노드를 왔다갔다 하기도 하며 컨테이너를 없애버리고, 직접 컨테이너에 접근하여 로그를 확인하는거 자체가 번거롭기때문이다.  
팟에서 발생하는 로그는 `kubectl logs -f podsname` 으로 실시간 테일링이 가능하지만, 어디까지나 실시간일 뿐이고 시간이 지나면 못보게 되거나하는 일이 생긴다.  
로그는 컨테이너 로그, 시스템 컴포넌트 로그가 있는데  
- 시스템 컴포넌트 로그  
    사용자가 제어하는게 아닌 시스템이 제어하는 컴포넌트, 그 중 컨테이너 기반으로 동작하지 않는 것.  
    컴포넌트마다 로그 위치나 설정이 제각각  
- 컨테이너 로그  
    컨테이너 런타임(containerd, cri-o, docker 등)안에서 발생하는 로그는 런타임 자체가 담당하고, 컨테이너 내부에서 stdout, stderr 의 표준 출력장치를 이용해 로그를 출력한다면 런타임이 로그 드라이버로 리다이렉트 하는 방식으로 수집한다고 한다.  
    드라이버는 syslog, json-file등이 있다.  
    간단히 `docker inspect 컨테이너ID` 로 출력되는 LogConfig 값으로 확인 가능  


### 모니터링 
위의 로그를 사용자가 의미있게 확인하기 위해서는 모니터링 시스템이 필요하다.  
흔히 ELK를 사용하는데, 책에서는 엘라스틱 서치와 키바나를 이용한 방법을 보여준다.  
그리고 로그 스태시(로그수집기)를 대체하여 플루언트디를 사용할수 있음.  
마찬가지로 키바나를 대체해 스턴(stern)을 이용해 시각화하는 방법도 있음.  
모니터링 툴은 아니지만 kubectl 을 대체하는 웹UI인 쿠버네티스 대시보드도 있다.  
이것도 마찬가지로 팟으로 동작하므로 yml로 정의해야함. 

로그 드라이버와 로그 수집기는 다르다. 
로그 드라이버는 컨테이너 런타임 레벨에서 동작하며 컨테이너가 동작하고 있는 노드에 파일로 로그를 저장하고,  
로그 수집기는 각 노드에 저장된 이 파일을 수집하여 중앙집중형으로 저장하고 관리한다.
주로 시각화 도구와 협업하여 사용자에게 보여주는 용도.


### 클러스터 메트릭 모니터링
로그와 마찬가지로 메트릭 수집 및 모니터링 툴이 있다.  
책에서 메트릭은 두가지로 소개하는데,   
- 시스템 메트릭  
    노드나 컨테이너의 cpu, 메모리 사용량 등 시스템 관련의 숫자  
- 서비스 메트릭   
    응답시간이나 에러응답, 정상응답갯수 등 서비스 관련 숫자  

주로 클러스터의 성능 측정이나 가용성 모니터링을 위해 사용됨.  
로그도 수집기가 필요한것처럼 메트릭도 서버가 필요함. kubelet에게서 정보를 수집해 api로 제공한다고 함.
예전에는 힙스터를 사용하였으나, 지금은 매트릭 서버로 대체되었음.  
그 외 유명한 프로메테우스나 데이터독, sysdig 등을 사용하는 방법도 있다.  
메트릭도 시각화 모니터링 툴이 있으며 책에서는 키바나와 그라파나를 소개함.

메트릭/로그 수집기와 시각화 도구는 별개이다.  
메트릭 수집도구는 프로메테우스, 시각화 도구는 그라파나, 이 두가지를 동시에 수행하는 데이터 독이 있다.  


### 오토스케일링 
Horizontal Pod Autoscaler HPA가 있다.  
말 그대로 주기적으로 실행되며 조건에 맞는다면 오토스케일링을 수행하여 팟의 갯수를 늘려주는 작업을 수행한다.  
책에서는 cpu만 가지고 정의했지만 실제로는 메모리, 디스크IO, 네트워크IO등의 기준을 통해서도 오토스케일링을 수행할수 있다.  
위에서 설명한 메트릭의 기준을 가지고 미리 정의한 타겟(예를들면 책에서 설명한것처럼 cpu 50% 같은 수치)에 도달하면 팟의 갯수를 늘리거나 반대로 줄인다던지를 수행함.  
물론 여기도 제한사항을 둘수있다. 실제 노드의 리소스는 제한적이므로.  
이미 배포가 된 디플로이먼트나 컨트롤러에도 HPA를 적용할수 있음.


### 사용자 정의 자원
프로그래밍 언어에서 원래 있던 자료형을 재정의 하는것처럼 쿠버네티스에서도 자원 재정의를 지원함.  
조건이 있다며 설명하지만 별거 없고 핵심은 기존 자원을 사용하듯 쓸수 있어야한다 가 핵심  
다시말하자면 기존의 자원을 사용하듯 구조화된 yml로 정의하고, cli를 이용할수 있으며, 자원에 대한 crud가 가능해야한다는것.  
Custom Resource Definitions CRD를 이용하면 된다. 근데 CRD자체도 yml로 정의해야함.  
사용자 정의 자원을 만들기 위해서 CRD를 yml로 정의한다음 쿠버네티스가 알수 있도록 등록하고 이후에 사용자 정의 자원을 사용하기 위해서 yml을 만들어 apply 해야한다.  
CRD에 사용되는 yml은 스키마 정의에 가깝다. 유효성을 검사하고, 네임스페이스인지 클러스터자원인지, 부를때는 어떤 이름으로 부를지를 정의한다.


### 워드프레스 배포 
책의 구성은 로컬 1대만 노드로 사용한다.  
- mysql   
    클러스터 내 mysql을 띄운다. 디플로이먼트를 이용함.  
    앱(워드프레스)에서 접근가능하도록 서비스를 띄운다. clusterIp 타입이면 충분함.  
    mysql에서 사용할 PV를 미리 작성하고, mysql yml 정의시 PVC를 같이 작성하여 사용한다.  
- 워드프레스  
    앞서 띄운 mysql 파드를 사용하는 앱  
    마찬가지로 디플로이먼트와 서비스를 사용하되, 서비스는 외부에서 접근이 가능하도록 NodePort로 정의한다. 

앞서 설명한대로 이미 배포된 디플로이먼트나 컨트롤러도 HPA가 적용이 가능하다.  
spec.scaleTargetRef 설정으로 이미 만들어진 디플로이먼트에 HPA 를 적용하여 오토스케일링한다.


### 헬름
yml을 관리하기 위한 외부 툴  
yml 템플릿의 집합을 차트라고 하고, 릴리즈는 이것을 실제로 실행한 인스턴스  
쿠버네티스의 일부는 아니며, brew따위로 따로 설치해야함.  
차트를 위한 git repo나 docker hub처럼 퍼블릭 레파지토리가 존재함.  
물론 넥서스처럼 프라이빗 레파지토리를 만들수도 있고.  
기본은 stable 이라고 하는데, 이미 많이 정의된 차트를 볼수있다.  `helm repo list`  
실제 차트를 사용하기 위해서 `helm install 차트명` 으로 바로 실행이 가능하며,  
바로 사용하지 않고 yml을 수정하기 위해서 `helm fetch 차트명` 으로 차트를 다운로드하고, 압축을 풀어 커스텀할수있다. 

